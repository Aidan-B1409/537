{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Detection\n",
    "---\n",
    "There are 10 images given in the ***images_corrected*** folder. Your task is to detect ***200*** keypoints for each of them using the SIFT detector.\n",
    "\n",
    "Below is the tutorial to follow for generating SIFT keypoints:\n",
    "\n",
    "SIFT: https://docs.opencv.org/3.3.0/da/df5/tutorial_py_sift_intro.html\n",
    "\n",
    "Let's take a look at these images first!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"images_corrected\"\n",
    "sifts = {}\n",
    "if os.path.exists(img_dir):\n",
    "    if os.listdir(img_dir) is []:\n",
    "        print(\"No images!\")\n",
    "        exit(0)\n",
    "    num_img = len(os.listdir(img_dir))\n",
    "    for img in os.listdir(img_dir):\n",
    "        if not img.endswith(\"jpg\"):\n",
    "            continue\n",
    "        image_dir = os.path.join(img_dir, img)\n",
    "        image = cv2.imread(image_dir)\n",
    "        gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures=200)\n",
    "        kp = sift.detect(gray,None)\n",
    "        sifts[img] = torch.Tensor(np.array([x.pt for x in kp][:200]))\n",
    "        img=cv2.drawKeypoints(gray,kp,image,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"image folder not exists!\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200, 2])\n"
     ]
    }
   ],
   "source": [
    "sorted_sifts = {k: v for k, v in sorted(sifts.items(), key=lambda x: int(x[0].split('.')[0]))}\n",
    "total_sifts = torch.stack(tuple(sorted_sifts.values()), dim=0)\n",
    "print(total_sifts.shape)\n",
    "torch.save(total_sifts, 'SIFT.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Keypoints:\n",
    "Upon running SIFT on the 10 images, for each image, the 200 highest-response SIFT keypoints should be kept. These keypoints should be saved as a torch tensor of size (10 x 200 x 2) in a file called \"SIFT.pth\". Please ensure that the tensor stores the 10 images in order by image name (i.e., 1-10.jpg) and that the 200 keypoints are sorted from highest response to lowest response.\n",
    "\n",
    "Note that the detected keypoints are represented as x and y coordinates in the image. For example, 10 keypoints of image ***3.jpg*** are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keypoints = list([(10.0, 10),  (16.0, 15.5), (15, 16), (1585, 16), (15, 1024), (100, 106), (150, 160), (715, 716), (315, 916), (815, 640)])\n",
    "keypoints = [(1040.0224609375, 300.8042907714844), (399.89947509765625, 235.52102661132812), (1011.2779541015625, 283.0083923339844), (950.94677734375, 291.58880615234375), (333.86993408203125, 234.14942932128906), (949.2930297851562, 488.50238037109375), (1006.3855590820312, 481.1703796386719), (933.950927734375, 306.55450439453125), (1006.9017333984375, 275.14898681640625), (1007.01953125, 288.50469970703125)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can obtain the patches with these keypoints using the following helper function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatches(kps, img, size=32, num=500):\n",
    "    res = torch.zeros(num, 1, size, size)\n",
    "    if type(img) is np.ndarray:\n",
    "        img = torch.from_numpy(img)\n",
    "    h, w = img.shape      # note: for image, the x direction is the verticle, y-direction is the horizontal...\n",
    "    for i in range(num):\n",
    "        cx, cy = kps[i]\n",
    "        cx, cy = int(cx), int(cy)\n",
    "        dd = int(size/2)\n",
    "        xmin, xmax = max(0, cx - dd), min(w, cx + dd ) \n",
    "        ymin, ymax = max(0, cy - dd), min(h, cy + dd ) \n",
    "        \n",
    "        xmin_res, xmax_res = dd - min(dd,cx), dd + min(dd, w - cx)\n",
    "        ymin_res, ymax_res = dd - min(dd,cy), dd + min(dd, h - cy)\n",
    "\n",
    "        cropped_img = img[ymin: ymax, xmin: xmax]\n",
    "        ch, cw = cropped_img.shape\n",
    "        res[i, 0, ymin_res: ymin_res+ch, xmin_res: xmin_res+cw] =  cropped_img\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot these patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([200, 1, 32, 32])\n",
      "torch.Size([10, 200, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"images_corrected\"\n",
    "all_patches = []\n",
    "if os.path.exists(img_dir):\n",
    "    if os.listdir(img_dir) is []:\n",
    "        print(\"No images!\")\n",
    "        exit(0)\n",
    "    num_img = len(os.listdir(img_dir))\n",
    "    for img in os.listdir(img_dir):\n",
    "        if not img.endswith(\"jpg\"):\n",
    "            continue\n",
    "        image_dir = os.path.join(img_dir, img)\n",
    "        image = cv2.imread(image_dir)\n",
    "        gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        patches = getPatches(sorted_sifts[img], gray,size=32, num=200)\n",
    "        for patch in patches:\n",
    "            im = patch[0].numpy()\n",
    "            # plt.imshow(im)\n",
    "            # plt.show()\n",
    "        print(patches.shape)\n",
    "        all_patches.append(patches)\n",
    "\n",
    "all_patches = torch.stack(all_patches, dim=0)\n",
    "print(all_patches.shape)\n",
    "output_dir = \"SIFT_patches.pth\"         # modify it to SIFT_patches.pth\n",
    "torch.save(all_patches, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images_corrected/3.jpg')\n",
    "gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "# gray = img[:, :, 0]\n",
    "patches = getPatches(keypoints, gray,size=32, num=10)\n",
    "for patch in patches:\n",
    "    im = patch[0].numpy()\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the patches with PyTorch\n",
    "You will extract 32x32 image patches for each of the 200 keypoints in each image. Theses patches will be saved as a torch tensor of size (10 x 200 x 1 x 32 x 32) in a file called \"SIFT_patches.pth\". Here 10 refers to the number of images (they should be stored sorted by image name, e.g, 1-10.jpg), 200 corresponds to the number of keypoints, and 1x32x32 corresponds to the gray scale image patch around a keypoint.\n",
    "\n",
    "An example of how to save such a tensor is shown below using the example extracted patches, where the tensor returned by ***getPatches()*** is the one that you should store in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patches = []\n",
    "all_patches.append(patches)\n",
    "all_patches = torch.stack(all_patches, dim=0)\n",
    "output_dir = \"patches.pth\"         # modify it to SIFT_patches.pth\n",
    "torch.save(all_patches, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with your saved patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([10, 200, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_patches = torch.load(output_dir)\n",
    "print(type(test_patches))\n",
    "print(test_patches.shape)\n",
    "# your tensor for each should have size of [10, 200, 1, 32, 32]; where 10 means 10 images (in the order 1-10), 200 means 200 keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
